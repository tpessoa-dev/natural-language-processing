{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 4-6 IA PLN - Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ont8JX9spNZz"
      },
      "source": [
        "# **NLTK**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtRjU5tRYZQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f411d3-605f-438c-8c67-05ecd54217f3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import tokenize\n",
        "\n",
        "texto_exemplo = \"Não sei se entendi como ler o conteúdo da view, então. \\\n",
        "Estou entendendo que a view contém um histórico das movimentações \\\n",
        "dos associados, certo?\"\n",
        "\n",
        "# Exemplo tokenização\n",
        "tokenize.word_tokenize(texto_exemplo, language=\"portuguese\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Não',\n",
              " 'sei',\n",
              " 'se',\n",
              " 'entendi',\n",
              " 'como',\n",
              " 'ler',\n",
              " 'o',\n",
              " 'conteúdo',\n",
              " 'da',\n",
              " 'view',\n",
              " ',',\n",
              " 'então',\n",
              " '.',\n",
              " 'Estou',\n",
              " 'entendendo',\n",
              " 'que',\n",
              " 'a',\n",
              " 'view',\n",
              " 'contém',\n",
              " 'um',\n",
              " 'histórico',\n",
              " 'das',\n",
              " 'movimentações',\n",
              " 'dos',\n",
              " 'associados',\n",
              " ',',\n",
              " 'certo',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReDnaw00nJid"
      },
      "source": [
        "Documentação nltk.tokenize: https://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "Exemplos em português: http://www.nltk.org/howto/portuguese_en.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPHAfEYkVp-f"
      },
      "source": [
        "POS-Tagger NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNORsKEaTbFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757f2083-4b23-4bad-a974-7a776501e5d7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "pos_tag(word_tokenize('O Hobbit - 7ª Ed. 2013  Produto NovoBilbo Bolseiro é um hobbit que', language=\"portuguese\"), tagset='universal')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('O', 'NOUN'),\n",
              " ('Hobbit', 'NOUN'),\n",
              " ('-', '.'),\n",
              " ('7ª', 'NUM'),\n",
              " ('Ed.', 'ADJ'),\n",
              " ('2013', 'NUM'),\n",
              " ('Produto', 'NOUN'),\n",
              " ('NovoBilbo', 'NOUN'),\n",
              " ('Bolseiro', 'NOUN'),\n",
              " ('é', 'NOUN'),\n",
              " ('um', 'ADJ'),\n",
              " ('hobbit', 'NOUN'),\n",
              " ('que', 'NOUN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGbw2K4UTdon"
      },
      "source": [
        "# **Sobre o corpus Floresta**\n",
        "\n",
        "---\n",
        "Conhecido como \"Floresta Sintática\". Conjunto de frases já analisadas sintáticamente e tageadas.\n",
        "\n",
        "https://www.linguateca.pt/Floresta/\n",
        "\n",
        "https://www.linguateca.pt/floresta/doc/VISLsymbolset-manual.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kySnyrT1c1sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd1c3f8-3f80-4b65-c674-6c14ca184668"
      },
      "source": [
        "from nltk.corpus import floresta\n",
        "import nltk\n",
        "nltk.download('floresta')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Package floresta is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mE1kjHprAkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110ee428-8bf7-46a1-a30f-4bd5cca97128"
      },
      "source": [
        "print(nltk.corpus.floresta.readme())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Portuguese Treebank\n",
            "\n",
            "Projecto Floresta Sinta(c)tica -- http://www.linguateca.pt/Floresta/\n",
            "Version 7.4  Distributed with permission.\n",
            "\n",
            "Penn Treebank format, available from http://linguateca.di.uminho.pt/FS/fs.html\n",
            "\n",
            "Key to tags (http://visl.sdu.dk/visl/pt/portsymbol.html)\n",
            "\n",
            "<ACC          direct object\n",
            "<ACC-PASS     passive use of pronoun 'se'\n",
            "<ADVS, <ADVO  adverbial argument\n",
            "<ADVL         adjunct adverbial\n",
            "<DAT          dative (indirect) object\n",
            "<FOC          focus marker (or right focus bracket)\n",
            "<OC           object complement\n",
            "<PASS         agent of passive\n",
            "<PIV          prepositional object\n",
            "<PRED         free (subject) predicative, right of main verb\n",
            "<SC           subject complement\n",
            "<SUBJ         subject\n",
            ">A            adverbial pre-adject (intensifier before adjective, adverb, pronoun or participle)\n",
            ">N            prenominal modifier\n",
            ">P            modifier of prepositional phrase (intensifier, operator or focus adverb)\n",
            ">S            modifier of clause (intensifier, operator or focus adverb)\n",
            "A<            adverbial post-adject (modifier or argument of adjective, adverb or participle)\n",
            "A<ADV         adverbial argument of attributive participle\n",
            "A<ADVL        adverbial adjunct of attributive participle\n",
            "A<PASS        agent of passive after attributive participle\n",
            "A<PIV         prepositional object of attributive participle\n",
            "A<SC          subject complement of attributive participle\n",
            "ACC>          accusative (direct) object\n",
            "ACC>-PASS     passive use of pronoun 'se'\n",
            "ACC>>         double-fronted accusative (direct) object before matrix verb\n",
            "ADVS>, ADVO>  adverbial argument\n",
            "ADVL          top node adverbial\n",
            "ADVL>         adjunct adverbial\n",
            "ADVL>A        adjunct adverbial before attributive participle\n",
            "ADVL>AS<      adjunct adverbial in averbal clause\n",
            "APP           identifying apposition\n",
            "AS<           clause body of averbal clause\n",
            "CO            co-ordinator\n",
            "COM           comparator (heading averbal clause)\n",
            "DAT>          dative (intransitive) object\n",
            "FAUX          finite auxiliary\n",
            "FMV           finite main verb\n",
            "FOC>          focus marker (or left focus bracket)\n",
            "IAUX          non-finite auxiliary\n",
            "IMV           non-finite main verb\n",
            "KOMP<         argument of comparative hook\n",
            "N<            postnominal modifier or argument\n",
            "N<PRED        postnominal (in-group) predicative (or non-identifying apposition)\n",
            "NPHR          top node noun phrase\n",
            "NUM<          second part of numeral chain\n",
            "OC>           object complement\n",
            "P<            argument of preposition\n",
            "PIV>          prepositional object\n",
            "PRD           predicator (heading averbal clause)\n",
            "PRED>         free (subject) predicative, left of main verb\n",
            "PREF          prefix (category being phased out)\n",
            "PRT-AUX<      auxiliary particle\n",
            "S<            statement predicative (sentence apposition)\n",
            "SC>           subject complement\n",
            "SUB           subordinator\n",
            "SUBJ>         subject\n",
            "SUBJ>>        double-fronted subject, with interfering matrix og quoting verb\n",
            "TOP           topic constituent\n",
            "VOK           vocative constituent\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqcDbl4mdodH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6538a2b-b037-495f-b1d9-818957d119d9"
      },
      "source": [
        "floresta.tagged_words()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Um', '>N+art'), ('revivalismo', 'H+n'), ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbMcbwmoMoPU"
      },
      "source": [
        "As tags consistem em algumas informações sintáticas, seguidas por um sinal de mais, seguido por uma tag convencional de POS Tag com a classificação morfológica. Vamos retirar o material antes do sinal de mais (+):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk7WlFDaHasW",
        "outputId": "0967198b-2e13-4da8-877a-427be1f4fa85"
      },
      "source": [
        "'+' in 'anderson dourado'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVsS_hn0j1O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4db5d3-d6bc-4469-e939-a8e0c0523260"
      },
      "source": [
        "'+' in 'anderson+dourado'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjE63qXqoA5W"
      },
      "source": [
        "# Função para simplificar a tag\n",
        "def simplifica_tag(t):\n",
        "  if \"+\" in t:\n",
        "    return t.split(\"+\")[1]\n",
        "  return t"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2SbIXeNmJf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6016b00e-b476-4abc-8864-98049406d5d1"
      },
      "source": [
        "# Exemplo 1\n",
        "tag_words = nltk.corpus.floresta.tagged_words() #type(twords)\n",
        "\n",
        "list_palavras = [] #type(list_palavras)\n",
        "tuple_dupla = () #type(tuple_dupla)\n",
        "\n",
        "for word, tag in tag_words:\n",
        "  tuple_dupla = (word.lower(), simplifica_tag(tag))\n",
        "  list_palavras.append(tuple_dupla)\n",
        "\n",
        "list_palavras[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('um', 'art'),\n",
              " ('revivalismo', 'n'),\n",
              " ('refrescante', 'adj'),\n",
              " ('o', 'art'),\n",
              " ('7_e_meio', 'prop'),\n",
              " ('é', 'v-fin'),\n",
              " ('um', 'art'),\n",
              " ('ex-libris', 'n'),\n",
              " ('de', 'prp'),\n",
              " ('a', 'art')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D8P4BWEdq45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099ad380-4e3c-4312-fd9e-55e24a7841b6"
      },
      "source": [
        "# Exemplo 2\n",
        "tag_words = nltk.corpus.floresta.tagged_words()\n",
        "tag_words\n",
        "\n",
        "tag_words = [(w.lower(),simplifica_tag(t)) for (w,t) in tag_words]\n",
        "\n",
        "tag_words[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('um', 'art'),\n",
              " ('revivalismo', 'n'),\n",
              " ('refrescante', 'adj'),\n",
              " ('o', 'art'),\n",
              " ('7_e_meio', 'prop'),\n",
              " ('é', 'v-fin'),\n",
              " ('um', 'art'),\n",
              " ('ex-libris', 'n'),\n",
              " ('de', 'prp'),\n",
              " ('a', 'art')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w3cHfOheAZ2"
      },
      "source": [
        "Exemplo de texto anotado (tagueado) no corpus Floresta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1EuM1jg_-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69d0489-84e2-45cd-f41d-67a76c640363"
      },
      "source": [
        "print(' '.join(word + '/' + tag for (word, tag) in tag_words[:10]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "um/art revivalismo/n refrescante/adj o/art 7_e_meio/prop é/v-fin um/art ex-libris/n de/prp a/art\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQQzXZP5pQ1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779ec72f-16b0-4bb6-80f7-965dd730c738"
      },
      "source": [
        "tag_sents = floresta.tagged_sents()\n",
        "tag_sents[:2]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj')],\n",
              " [('O', '>N+art'),\n",
              "  ('7_e_Meio', 'H+prop'),\n",
              "  ('é', 'P+v-fin'),\n",
              "  ('um', '>N+art'),\n",
              "  ('ex-libris', 'H+n'),\n",
              "  ('de', 'H+prp'),\n",
              "  ('a', '>N+art'),\n",
              "  ('noite', 'H+n'),\n",
              "  ('algarvia', 'N<+adj'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCbf1QPT1InW"
      },
      "source": [
        "# Contagem das tags e mantendo a estruturas das sentenças do corpus floresta\n",
        "from nltk.corpus import floresta\n",
        "from collections import Counter\n",
        "\n",
        "def simplifica_tag(t):\n",
        "  if \"+\" in t:\n",
        "    return t.split(\"+\")[1]\n",
        "  return t \n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "tag_sents = floresta.tagged_sents()\n",
        "tag_new_sents = []\n",
        "for sent in tag_sents:\n",
        "  new_sent = []\n",
        "  for (w,t) in sent:\n",
        "    tag = simplifica_tag(t)\n",
        "    new_sent.append((w.lower(), tag))\n",
        "    counter[tag] += 1\n",
        "  tag_new_sents.append(new_sent)\n",
        "\n",
        "#tag_sents[0]\n",
        "#tag_new_sents[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNc4oyTTRzrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0f6d24-30cb-41a8-aa63-94b191fa8037"
      },
      "source": [
        "#new_sent\n",
        "tag_new_sents[:2]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('um', 'art'), ('revivalismo', 'n'), ('refrescante', 'adj')],\n",
              " [('o', 'art'),\n",
              "  ('7_e_meio', 'prop'),\n",
              "  ('é', 'v-fin'),\n",
              "  ('um', 'art'),\n",
              "  ('ex-libris', 'n'),\n",
              "  ('de', 'prp'),\n",
              "  ('a', 'art'),\n",
              "  ('noite', 'n'),\n",
              "  ('algarvia', 'adj'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNmXhgOw1t7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d22e402-f256-4519-b45e-8bee053f4e9b"
      },
      "source": [
        "counter.most_common(5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('n', 40081), ('prp', 32442), ('art', 29360), ('v-fin', 15802), (',', 13444)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6mPWpv62Z0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6325a51-e456-4f61-a879-aaa49939fefe"
      },
      "source": [
        "# % dos substantivos\n",
        "counter.get('n') / sum(counter.values())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18919339916545513"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpQ87t8fgAzX"
      },
      "source": [
        "Verificamos que a tag mais comum é N. Essa será nossa referência a tag padrão (gold)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKnTSb5Z7uBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18701300-c2df-47b9-e905-fe4046fdb8e4"
      },
      "source": [
        "# Crianto uma tag default\n",
        "default_tagger = nltk.DefaultTagger('n')\n",
        "\n",
        "token_texto_exemplo = tokenize.word_tokenize(texto_exemplo, language=\"portuguese\")\n",
        "default_tagger.tag(token_texto_exemplo)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Não', 'n'),\n",
              " ('sei', 'n'),\n",
              " ('se', 'n'),\n",
              " ('entendi', 'n'),\n",
              " ('como', 'n'),\n",
              " ('ler', 'n'),\n",
              " ('o', 'n'),\n",
              " ('conteúdo', 'n'),\n",
              " ('da', 'n'),\n",
              " ('view', 'n'),\n",
              " (',', 'n'),\n",
              " ('então', 'n'),\n",
              " ('.', 'n'),\n",
              " ('Estou', 'n'),\n",
              " ('entendendo', 'n'),\n",
              " ('que', 'n'),\n",
              " ('a', 'n'),\n",
              " ('view', 'n'),\n",
              " ('contém', 'n'),\n",
              " ('um', 'n'),\n",
              " ('histórico', 'n'),\n",
              " ('das', 'n'),\n",
              " ('movimentações', 'n'),\n",
              " ('dos', 'n'),\n",
              " ('associados', 'n'),\n",
              " (',', 'n'),\n",
              " ('certo', 'n'),\n",
              " ('?', 'n')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZf9bpKwACxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc54f23-afd3-40ec-8449-03521793a795"
      },
      "source": [
        "# Analisando com base na tag padrão (tag ouro/gold)\n",
        "tag_sents = tag_new_sents\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('n')\n",
        "print(default_tagger.evaluate(tag_sents))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.18919339916545513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er6gXU5IhTkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9ee340-13ce-42c8-b8f8-39a9c35c0a9f"
      },
      "source": [
        "tag_sents = tag_new_sents\n",
        "#type(tag_sents) #len(tag_sents)\n",
        "#tag_sents[len(tag_sents)-1] #tag_sents[-1]\n",
        "\n",
        "tag_sents_treino = tag_sents[1000:]\n",
        "tag_sents_teste = tag_sents[:1000]\n",
        "\n",
        "tagger0 = nltk.DefaultTagger('n')\n",
        "print(tagger0.evaluate(tag_sents_teste))\n",
        "\n",
        "tagger1 = nltk.UnigramTagger(tag_sents_treino, backoff=tagger0)\n",
        "print(tagger1.evaluate(tag_sents_teste))\n",
        "\n",
        "tagger2 = nltk.BigramTagger(tag_sents_treino, backoff=tagger1)\n",
        "print(tagger2.evaluate(tag_sents_teste))\n",
        "\n",
        "tagger3 = nltk.TrigramTagger(tag_sents_treino, backoff=tagger2)\n",
        "print(tagger3.evaluate(tag_sents_teste))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.17800040072129833\n",
            "0.8740532959326788\n",
            "0.8900420757363254\n",
            "0.8887998397114807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e4lHBKC9xUB"
      },
      "source": [
        "\n",
        "Documentação dos métodos tag:\n",
        "  - https://www.nltk.org/api/nltk.tag.html\n",
        "\n",
        "Nos baseamos no métodos da documentação abaixo:\n",
        "  - Capítulo 5 - N-Gram Tagging: http://www.nltk.org/book/ch05.html\n",
        "  - Exemplo em português: http://www.nltk.org/howto/portuguese_en.html\n",
        "  - Outros corpus tageados: https://www.nltk.org/book/ch02.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAe-S-errcNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15007577-57be-41b7-ffc8-d8297eba2cdb"
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "\n",
        "joblib.dump(tagger3, 'tagger.pkl')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quA-JTPNOzcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26b4701-57fa-4d49-a5a7-4b965103c024"
      },
      "source": [
        "!ls -all -h"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 776K\n",
            "drwxr-xr-x 1 root root 4.0K May 10 16:34 .\n",
            "drwxr-xr-x 1 root root 4.0K May 10 16:29 ..\n",
            "drwxr-xr-x 4 root root 4.0K May  6 13:43 .config\n",
            "drwxr-xr-x 1 root root 4.0K May  6 13:44 sample_data\n",
            "-rw-r--r-- 1 root root 758K May 10 16:34 tagger.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVCddXw7sVsa"
      },
      "source": [
        "bla = joblib.load('tagger.pkl')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKiTgA9ysc0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5c7f61-9af1-46b7-8a1b-a3b2f830fda6"
      },
      "source": [
        "print(bla.evaluate(tag_sents_teste))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8887998397114807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmZdOuNCaV0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1f9a1a-ce1e-4f1a-bdca-863e66fbab43"
      },
      "source": [
        "texto_exemplo\n",
        "twords = tokenize.word_tokenize(texto_exemplo, language=\"portuguese\")\n",
        "bla.tag(twords)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Não', 'n'),\n",
              " ('sei', 'v-fin'),\n",
              " ('se', 'pron-pers'),\n",
              " ('entendi', 'n'),\n",
              " ('como', 'adv'),\n",
              " ('ler', 'v-inf'),\n",
              " ('o', 'art'),\n",
              " ('conteúdo', 'n'),\n",
              " ('da', 'n'),\n",
              " ('view', 'n'),\n",
              " (',', ','),\n",
              " ('então', 'adv'),\n",
              " ('.', '.'),\n",
              " ('Estou', 'n'),\n",
              " ('entendendo', 'v-ger'),\n",
              " ('que', 'conj-s'),\n",
              " ('a', 'art'),\n",
              " ('view', 'n'),\n",
              " ('contém', 'n'),\n",
              " ('um', 'art'),\n",
              " ('histórico', 'adj'),\n",
              " ('das', 'n'),\n",
              " ('movimentações', 'n'),\n",
              " ('dos', 'prop'),\n",
              " ('associados', 'n'),\n",
              " (',', ','),\n",
              " ('certo', 'adj'),\n",
              " ('?', '?')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI-YYFYUjwIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e1bb12-0006-46c8-ffee-5473aa1e9f34"
      },
      "source": [
        "#twords = tokenize.word_tokenize(texto_exemplo, language=\"portuguese\")\n",
        "tagger2.tag(twords)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Não', 'n'),\n",
              " ('sei', 'v-fin'),\n",
              " ('se', 'pron-pers'),\n",
              " ('entendi', 'n'),\n",
              " ('como', 'adv'),\n",
              " ('ler', 'v-inf'),\n",
              " ('o', 'art'),\n",
              " ('conteúdo', 'n'),\n",
              " ('da', 'n'),\n",
              " ('view', 'n'),\n",
              " (',', ','),\n",
              " ('então', 'adv'),\n",
              " ('.', '.'),\n",
              " ('Estou', 'n'),\n",
              " ('entendendo', 'v-ger'),\n",
              " ('que', 'conj-s'),\n",
              " ('a', 'art'),\n",
              " ('view', 'n'),\n",
              " ('contém', 'n'),\n",
              " ('um', 'art'),\n",
              " ('histórico', 'adj'),\n",
              " ('das', 'n'),\n",
              " ('movimentações', 'n'),\n",
              " ('dos', 'prop'),\n",
              " ('associados', 'n'),\n",
              " (',', ','),\n",
              " ('certo', 'adj'),\n",
              " ('?', '?')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iIx3MhZkl-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccac89f2-e8fb-4bd1-d6b2-9312e10a215a"
      },
      "source": [
        "tagger1.tag(twords)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Não', 'n'),\n",
              " ('sei', 'v-fin'),\n",
              " ('se', 'pron-pers'),\n",
              " ('entendi', 'n'),\n",
              " ('como', 'adv'),\n",
              " ('ler', 'v-inf'),\n",
              " ('o', 'art'),\n",
              " ('conteúdo', 'n'),\n",
              " ('da', 'n'),\n",
              " ('view', 'n'),\n",
              " (',', ','),\n",
              " ('então', 'adv'),\n",
              " ('.', '.'),\n",
              " ('Estou', 'n'),\n",
              " ('entendendo', 'v-ger'),\n",
              " ('que', 'pron-indp'),\n",
              " ('a', 'art'),\n",
              " ('view', 'n'),\n",
              " ('contém', 'n'),\n",
              " ('um', 'art'),\n",
              " ('histórico', 'adj'),\n",
              " ('das', 'n'),\n",
              " ('movimentações', 'n'),\n",
              " ('dos', 'prop'),\n",
              " ('associados', 'n'),\n",
              " (',', ','),\n",
              " ('certo', 'adj'),\n",
              " ('?', '?')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inKncgHXouUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f059aeb1-8503-4411-e5a0-56153d29d868"
      },
      "source": [
        "print(nltk.corpus.floresta.readme()[:1000])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Portuguese Treebank\n",
            "\n",
            "Projecto Floresta Sinta(c)tica -- http://www.linguateca.pt/Floresta/\n",
            "Version 7.4  Distributed with permission.\n",
            "\n",
            "Penn Treebank format, available from http://linguateca.di.uminho.pt/FS/fs.html\n",
            "\n",
            "Key to tags (http://visl.sdu.dk/visl/pt/portsymbol.html)\n",
            "\n",
            "<ACC          direct object\n",
            "<ACC-PASS     passive use of pronoun 'se'\n",
            "<ADVS, <ADVO  adverbial argument\n",
            "<ADVL         adjunct adverbial\n",
            "<DAT          dative (indirect) object\n",
            "<FOC          focus marker (or right focus bracket)\n",
            "<OC           object complement\n",
            "<PASS         agent of passive\n",
            "<PIV          prepositional object\n",
            "<PRED         free (subject) predicative, right of main verb\n",
            "<SC           subject complement\n",
            "<SUBJ         subject\n",
            ">A            adverbial pre-adject (intensifier before adjective, adverb, pronoun or participle)\n",
            ">N            prenominal modifier\n",
            ">P            modifier of prepositional phrase (intensifier, operator or focus adverb)\n",
            ">S            modifier of clause (intensifier, operator or focus adverb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpdWBDZlWrDw"
      },
      "source": [
        "# **TextBlob**\n",
        "\n",
        "---\n",
        "\n",
        "O TextBlob já oferente uma seria de recursos de PLN, como: marcação POS-Tag, extração de frases substantivas, análise de sentimentos, classificação, tradução e outras.\n",
        "\n",
        "- Documentação oficial: https://textblob.readthedocs.io/en/dev/index.html\n",
        "- Natural Language Basics with TextBlob - Allison Parrish: http://rwet.decontextualize.com/book/textblob/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4kC31vdIlza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4820641d-569d-484b-c587-82fdd3a0d1c6"
      },
      "source": [
        "!pip install textblob"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udjWzyToWuKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56c8349-0abc-4ad6-ef08-f696b38c777e"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "opinion = TextBlob(\"batman vs superman is a shit!\", analyzer=NaiveBayesAnalyzer())\n",
        "opinion.sentiment"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(classification='neg', p_pos=0.288629160063391, p_neg=0.7113708399366088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuKIO5dO5Mg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cc1805-52f2-4e12-b8a9-dcb3a687c794"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjTHSeM-SBMh"
      },
      "source": [
        "Outras funcionalidades do TextBlob, como: tradutor e pos-tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dYx4-xZWR5xs",
        "outputId": "9a2b29c5-40f7-4bd6-e541-b28ab13b6229"
      },
      "source": [
        "'''\n",
        "texto_exemplo = \"Não sei se entendi como ler o conteúdo da view, então. \\\n",
        "Estou entendendo que a view contém um histórico das movimentações \\\n",
        "dos associados, certo?\"\n",
        "'''\n",
        "texto_exemplo"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Não sei se entendi como ler o conteúdo da view, então. Estou entendendo que a view contém um histórico das movimentações dos associados, certo?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM5jokbDglMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee7d556-b79f-49cb-c555-d3f34b7da1d5"
      },
      "source": [
        "print(\"Frase original: \", texto_exemplo)\n",
        "\n",
        "tblob = TextBlob(texto_exemplo)\n",
        "\n",
        "print('Idioma: ',tblob.detect_language())\n",
        "\n",
        "en_tblob = tblob.translate(to='en')\n",
        "\n",
        "print(\"Traduzido: \", en_tblob)\n",
        "\n",
        "print(\"Tags: \", en_tblob.tags)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frase original:  Não sei se entendi como ler o conteúdo da view, então. Estou entendendo que a view contém um histórico das movimentações dos associados, certo?\n",
            "Idioma:  pt\n",
            "Traduzido:  I don't know if I understood how to read the contents of the view, then. I understand that the view contains a history of the members' movements, right?\n",
            "Tags:  [('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('if', 'IN'), ('I', 'PRP'), ('understood', 'VBD'), ('how', 'WRB'), ('to', 'TO'), ('read', 'VB'), ('the', 'DT'), ('contents', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('view', 'NN'), ('then', 'RB'), ('I', 'PRP'), ('understand', 'VBP'), ('that', 'IN'), ('the', 'DT'), ('view', 'NN'), ('contains', 'VBZ'), ('a', 'DT'), ('history', 'NN'), ('of', 'IN'), ('the', 'DT'), ('members', 'NNS'), (\"'\", 'POS'), ('movements', 'NNS'), ('right', 'RB')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJeCWBioc6Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cfa165d-b3a4-4714-8a0f-2ddb5e8d6184"
      },
      "source": [
        "text = \"As aulas da USP são muito chatas\"\n",
        "\n",
        "tblob = TextBlob(text)\n",
        "\n",
        "en_text = tblob.translate(to='en')\n",
        "\n",
        "en_text"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"USP classes are very boring\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_pip7sRdLV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c907a3e3-6ab4-41fd-884a-7802a3e3b2d4"
      },
      "source": [
        "en = TextBlob(en_text.string, analyzer=NaiveBayesAnalyzer())\n",
        "\n",
        "en.sentiment"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(classification='neg', p_pos=0.2782420508972421, p_neg=0.7217579491027579)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i66U-_tP3Yv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abff09a-96b3-4a34-cfd7-d92e8f3e0641"
      },
      "source": [
        "# Exemplo com uma forma de deixar todo o texto no mesmo indioma\n",
        "en_text = \"As aulas da USP são muito chatas. My god!\"\n",
        "\n",
        "#Exemplo 1\n",
        "tblob = TextBlob(en_text)\n",
        "pt_text = tblob.translate(from_lang='en', to='pt')\n",
        "\n",
        "pt_text"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"As aulas da USP são muito chatas. Meu Deus!\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz9WE7opSIFp"
      },
      "source": [
        "Outro tradutor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUhLrK37I4D-"
      },
      "source": [
        "!pip install translate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hVs811vvx7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "681e5b19-82c9-4169-94c8-c10e085d09fe"
      },
      "source": [
        "from translate import Translator\n",
        "\n",
        "#Exemplo 2 usando outro pacote o \"translate\"\n",
        "en_text = \"As aulas da USP são muito chatas. My god!\"\n",
        "\n",
        "translator = Translator(to_lang=\"pt\")\n",
        "pt_text = translator.translate(en_text)\n",
        "\n",
        "pt_text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'As aulas da USP são muito chatas. Meu Deus!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW_uBucJSPmo"
      },
      "source": [
        "### Treinando um modelo do TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh1_3CoNSMV4",
        "outputId": "6b48a5ef-d227-4bb9-99c2-e32f245a2fc4"
      },
      "source": [
        "# Treinamento do Classificador TextBlob ()\n",
        "import pandas as pd\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Demostração de validação\n",
        "df_treino = pd.DataFrame({\n",
        "    'text': [\n",
        "      'Sobre MBA ? Eu gostei muito do MBA da FIAP',\n",
        "      'O MBA da FIAP pode melhorar, não gostei muito',\n",
        "      'O curso pode melhorar'\n",
        "    ],\n",
        "    'class': [\n",
        "        'positivo',\n",
        "        'negativo',\n",
        "        'negativo'\n",
        "    ]})\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = NaiveBayesClassifier(df_treino.values.tolist())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0faab2R6SMTg"
      },
      "source": [
        "# Demostração de validação\n",
        "df_teste = pd.DataFrame({\n",
        "    'text': [\n",
        "      'O curso pode melhorar'\n",
        "    ],\n",
        "    'class': [\n",
        "        'negativo'\n",
        "    ]})\n",
        "\n",
        "subset_teste = df_teste[['text','class']]\n",
        "tuples_teste = [tuple(x) for x in subset_teste.values]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUVP78mGSMOK",
        "outputId": "094631ae-10a9-4123-b7f8-a26a942178f2"
      },
      "source": [
        "accuracy = model.accuracy(df_teste[['text','class']].values.tolist())\n",
        "\n",
        "y_predito = model.classify(tuples_teste[0][0])\n",
        "y_predito\n",
        "\n",
        "print('Acurácia: {}'.format(accuracy))\n",
        "print(y_predito)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia: 1.0\n",
            "negativo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vlBYSFSSMLm",
        "outputId": "83c2c865-b83a-4ed0-8fb9-bb466b6cd5fa"
      },
      "source": [
        "# outra forma\n",
        "texto = 'O curso pode melhorar'\n",
        "\n",
        "blob = TextBlob(texto,classifier=model)\n",
        "\n",
        "print('Predição: {}'.format(blob.classify()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predição: negativo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5bpdFRLSc3n"
      },
      "source": [
        "Aplicando o modelo de classificação do TextBlob na nossa base de produtos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRpPPkhcSbmr",
        "outputId": "4273db91-08a0-433c-9271-2dfec5ed6eaa"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# carrega os dados\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8').sample(1000)\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']\n",
        "\n",
        "# divisão das amostras em treino e teste\n",
        "df_treino, df_teste = train_test_split(\n",
        "      df, \n",
        "      test_size = 0.3, \n",
        "      random_state = 42\n",
        "  )\n",
        "\n",
        "# Preparando os dados para o modelo\n",
        "subset_treino = df_treino[['texto','categoria']]\n",
        "tuples_treino = [tuple(x) for x in subset_treino.values]\n",
        "#tuples_treino[0]\n",
        "subset_teste = df_teste[['texto','categoria']]\n",
        "tuples_teste = [tuple(x) for x in subset_teste.values]\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = NaiveBayesClassifier(tuples_treino)\n",
        "# ou\n",
        "#model = NaiveBayesClassifier(df_treino[['texto','categoria']].values.tolist())\n",
        "print(model)\n",
        "\n",
        "# validação\n",
        "accuracy = model.accuracy(tuples_teste)\n",
        "print('Precisão: {}'.format(accuracy))\n",
        "\n",
        "# classificação (predição)\n",
        "def fn_classificacao():\n",
        "  for i in range(len(tuples_teste)):\n",
        "    classification = model.classify(tuples_teste[i][0])\n",
        "    y_predito.append(classification)\n",
        "\n",
        "y_predito = []\n",
        "fn_classificacao()\n",
        "\n",
        "df_teste['categoria_predito'] = y_predito\n",
        "\n",
        "print(df_teste[['categoria','categoria_predito']].head(5))\n",
        "\n",
        "#y_predito = model.classify(df_teste[['texto','categoria']].values.tolist())\n",
        "#y_predito"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<NaiveBayesClassifier trained on 482 instances>\n",
            "Precisão:0.855072463768116\n",
            "      categoria categoria_predito\n",
            "1984  brinquedo         brinquedo\n",
            "632       livro         brinquedo\n",
            "121       livro         brinquedo\n",
            "1411  brinquedo         brinquedo\n",
            "2285  maquiagem         maquiagem\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBpxl3BOVYs7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZFcDaAUp-f2"
      },
      "source": [
        "# **SpaCy**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii4EiP-ciUGo"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZERDUlN--4q"
      },
      "source": [
        "# Outros modelos do pacote SpaCy\n",
        "#!python -m spacy download en\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!python -m spacy download pt_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wikgr3q7zs8W"
      },
      "source": [
        "Os modelos e linguagens que o SpaCy suporta: https://spacy.io/usage/models\n",
        "\n",
        "Detalhes do modelo em português: https://spacy.io/models/pt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxv4W49BqDC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26c7341-51ce-42ac-a234-770b703e49a3"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "doc = nlp(u'Você encontrou o livro que eu te falei, Carla?')\n",
        "\n",
        "print(type(doc))\n",
        "print([token.orth_ for token in doc])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "['Você', 'encontrou', 'o', 'livro', 'que', 'eu', 'te', 'falei', ',', 'Carla', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkxmhVTe16p4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9317780-f374-4e07-92ac-d0120b606782"
      },
      "source": [
        "print([token.text for token in doc])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Você', 'encontrou', 'o', 'livro', 'que', 'eu', 'te', 'falei', ',', 'Carla', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvXJTq8ofR3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451153c4-1998-4758-ad67-6d4a5f361d64"
      },
      "source": [
        "type(doc[0].orth_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXLpdMNH3XxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1574d6c-15e1-4563-ac15-14d5de791eaa"
      },
      "source": [
        "doc[0].orth_"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Você'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kihe7mhKrZFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51f44bb-8d57-43a0-a7af-6bc5547e9e0d"
      },
      "source": [
        "[(token.text, token.orth_, token.pos_, token.dep_, spacy.explain(token.pos_)) for token in doc]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Você', 'Você', 'PRON', 'nsubj', 'pronoun'),\n",
              " ('encontrou', 'encontrou', 'VERB', 'ROOT', 'verb'),\n",
              " ('o', 'o', 'DET', 'det', 'determiner'),\n",
              " ('livro', 'livro', 'NOUN', 'obj', 'noun'),\n",
              " ('que', 'que', 'PRON', 'obj', 'pronoun'),\n",
              " ('eu', 'eu', 'PRON', 'nsubj', 'pronoun'),\n",
              " ('te', 'te', 'VERB', 'obj', 'verb'),\n",
              " ('falei', 'falei', 'VERB', 'acl:relcl', 'verb'),\n",
              " (',', ',', 'PUNCT', 'punct', 'punctuation'),\n",
              " ('Carla', 'Carla', 'PROPN', 'conj', 'proper noun'),\n",
              " ('?', '?', 'PUNCT', 'punct', 'punctuation')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4IKxyc0ghgX"
      },
      "source": [
        "Uma lista completa de dependências sintáticas pode ser vista em: https://spacy.io/api/annotation#dependency-parsing \n",
        "\n",
        "Um bom material complementar para dependências sintáticas pode ser vista no [\"Stanford typed dependencies manual\"](https://nlp.stanford.edu/software/dependencies_manual.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGGhkVL5b_kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f7e49d-a324-4979-f650-2fbd26a85bca"
      },
      "source": [
        "doc"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Você encontrou o livro que eu te falei, Carla?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX_bRMWpKE3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065170f2-5454-4445-8d3f-f552e8667276"
      },
      "source": [
        "[token.lemma_ for token in doc]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Você',\n",
              " 'encontrar',\n",
              " 'o',\n",
              " 'livrar',\n",
              " 'que',\n",
              " 'eu',\n",
              " 'te',\n",
              " 'falar',\n",
              " ',',\n",
              " 'Carla',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD_Mmy2YKE3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2034bf5b-da75-4ba8-f9d0-b3de09fd1cee"
      },
      "source": [
        "doc = nlp(u'encontrardes, encontraram, encontrarão, encontrariam, encontrasse, encontraria')\n",
        "print([token.lemma_ for token in doc])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['encontrar', ',', 'encontrar', ',', 'encontrar', ',', 'encontrar', ',', 'encontrar', ',', 'encontrar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIbo-wEj5JUn"
      },
      "source": [
        "Reconhecimento de entidades nomeadas - NER: Named-Enntity Recognition.\n",
        "\n",
        "Utilizada para reconhecer pessoas, locais, empresas, datas, numerais e outros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NPR6lntKE3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43672d44-cb0a-4e23-d679-a20b2508929a"
      },
      "source": [
        "doc = nlp(u'Pelé um dos melhores escritores do Brasil, \\\n",
        "foi o primeiro presidente da Academia Brasileira de Letras')\n",
        "\n",
        "print(doc.ents)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Pelé, Brasil, Academia Brasileira de Letras)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9APZK79KE3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4aaf65-e7f8-45fa-d0e9-fc113fb2d405"
      },
      "source": [
        "[(entity, entity.label_, spacy.explain(entity.label_)) for entity in doc.ents]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Pelé, 'PER', 'Named person or family.'),\n",
              " (Brasil, 'LOC', 'Non-GPE locations, mountain ranges, bodies of water'),\n",
              " (Academia Brasileira de Letras,\n",
              "  'ORG',\n",
              "  'Companies, agencies, institutions, etc.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc0zcc6b_YZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46b4401-0751-44aa-c10d-a04a6e686731"
      },
      "source": [
        "[(entity, entity.pos_) for entity in doc]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Pelé, 'PROPN'),\n",
              " (um, 'NUM'),\n",
              " (dos, 'ADP'),\n",
              " (melhores, 'ADJ'),\n",
              " (escritores, 'NOUN'),\n",
              " (do, 'ADP'),\n",
              " (Brasil, 'PROPN'),\n",
              " (,, 'PUNCT'),\n",
              " (foi, 'VERB'),\n",
              " (o, 'DET'),\n",
              " (primeiro, 'ADJ'),\n",
              " (presidente, 'NOUN'),\n",
              " (da, 'ADP'),\n",
              " (Academia, 'PROPN'),\n",
              " (Brasileira, 'PROPN'),\n",
              " (de, 'ADP'),\n",
              " (Letras, 'PROPN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru_7c5mpia4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef37d3e-cc79-4692-cca7-ab8178a07500"
      },
      "source": [
        "doc8 = nlp(u'Google investirá 6 milhões de dólares')\n",
        "\n",
        "for token in doc8:\n",
        "    print(token.text, end =' | ')\n",
        "\n",
        "print('\\n----')\n",
        "\n",
        "for ent in doc8.ents:\n",
        "    print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google | investirá | 6 | milhões | de | dólares | \n",
            "----\n",
            "Google - ORG - Companies, agencies, institutions, etc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzVAI0Z6KE3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671f2096-9f09-4136-beb1-3145e7ec41da"
      },
      "source": [
        "doc4 = nlp(u'Esta é a primeira sentença. Sr. esta é a segunda sentença. Esta é a terceira. Você já entendeu né?')\n",
        "\n",
        "for sent in doc4.sents:\n",
        "    print(sent)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Esta é a primeira sentença.\n",
            "Sr. esta é a segunda sentença.\n",
            "Esta é a terceira.\n",
            "Você já entendeu né?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6ta7C_7D2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59a4b7f-cebb-4c70-f325-cb4d2788ac96"
      },
      "source": [
        "doc4[6]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sr."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E6jPE6yh38K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f235159f-3e10-4f15-e761-056ab94e1eec"
      },
      "source": [
        "print(doc4[6])\n",
        "print(doc4[6].is_sent_start)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sr.\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPqiHvM7h8Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e07a82f-d4a8-4293-b26c-ac18e8e7f80d"
      },
      "source": [
        "stop = nlp.Defaults.stop_words\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'estava', 'qualquer', 'lugar', 'alguns', 'vêm', 'em', 'sua', 'das', 'teve', 'tanta', 'pegar', 'bem', 'estado', 'teus', 'catorze', 'caminho', 'menos', 'aquele', 'cinco', 'neste', 'lhe', 'cá', 'outros', 'nossos', 'possível', 'cedo', 'ora', 'bastante', 'daquela', 'fazeis', 'fim', 'muito', 'dizer', 'vários', 'pôde', 'quero', 'era', 'nossas', 'sete', 'ter', 'sexta', 'sétima', 'estes', 'através', 'eu', 'aqueles', 'com', 'próximo', 'pela', 'já', 'eles', 'sabe', 'não', 'vai', 'fazemos', 'nos', 'tentei', 'põem', 'pelos', 'tive', 'comprido', 'coisa', 'estive', 'os', 'faço', 'embora', 'tão', 'foi', 'corrente', 'ambas', 'diz', 'estou', 'aí', 'até', 'algo', 'exemplo', 'inclusive', 'próxima', 'temos', 'parece', 'também', 'contra', 'porquanto', 'nas', 'sempre', 'vez', 'sim', 'tenho', 'ligado', 'somos', 'pouca', 'fazia', 'entre', 'de', 'for', 'mesmo', 'uma', 'mil', 'vinda', 'nenhuma', 'possivelmente', 'põe', 'usa', 'mais', 'primeira', 'apontar', 'seus', 'comprida', 'querem', 'terceira', 'valor', 'obrigada', 'isso', 'algumas', 'vais', 'cuja', 'desde', 'elas', 'faz', 'custa', 'geral', 'naquele', 'nesta', 'questão', 'vocês', 'deste', 'isto', 'favor', 'conhecido', 'dá', 'menor', 'tudo', 'breve', 'terceiro', 'vossa', 'todas', 'tipo', 'tivestes', 'foste', 'pouco', 'tentaram', 'se', 'esse', 'usar', 'diante', 'vosso', 'irá', 'suas', 'você', 'pois', 'vens', 'fora', 'naquela', 'final', 'vinte', 'aqui', 'estivestes', 'adeus', 'desta', 'fazem', 'podem', 'último', 'ademais', 'minha', 'números', 'à', 'vós', 'cento', 'quinta', 'quieta', 'devem', 'oitavo', 'contudo', 'vão', 'foram', 'acerca', 'povo', 'momento', 'és', 'nunca', 'grandes', 'fomos', 'na', 'sou', 'lado', 'nesse', 'minhas', 'próprio', 'demais', 'mas', 'então', 'tu', 'ver', 'dessa', 'dezassete', 'agora', 'baixo', 'nem', 'novo', 'fazer', 'ir', 'fui', 'somente', 'esta', 'têm', 'tal', 'quatro', 'sistema', 'assim', 'dezanove', 'no', 'nível', 'cada', 'relação', 'sob', 'estão', 'mês', 'ambos', 'nosso', 'dezasseis', 'tiveste', 'ponto', 'quinze', 'todo', 'fez', 'certeza', 'sétimo', 'segundo', 'inicio', 'nessa', 'segunda', 'máximo', 'num', 'tente', 'boa', 'parte', 'lá', 'debaixo', 'ainda', 'porque', 'te', 'nossa', 'veja', 'tivemos', 'disso', 'as', 'uns', 'depois', 'bom', 'tens', 'conhecida', 'saber', 'quieto', 'iniciar', 'este', 'muitos', 'tiveram', 'vos', 'todos', 'primeiro', 'deve', 'zero', 'posição', 'dois', 'número', 'sei', 'ao', 'duas', 'apoio', 'quarto', 'está', 'às', 'seu', 'perto', 'são', 'sois', 'meses', 'portanto', 'dar', 'enquanto', 'tarde', 'como', 'do', 'tanto', 'apenas', 'apoia', 'tua', 'estará', 'oitava', 'meio', 'talvez', 'longe', 'esteve', 'grande', 'qual', 'forma', 'quê', 'quarta', 'daquele', 'pontos', 'pode', 'essa', 'sexto', 'sobre', 'pelas', 'por', 'novas', 'novos', 'poder', 'tais', 'essas', 'tentar', 'antes', 'estiveram', 'des', 'quanto', 'fostes', 'quer', 'grupo', 'onde', 'certamente', 'esses', 'só', 'ele', 'toda', 'outras', 'aquelas', 'após', 'para', 'atrás', 'estar', 'aquilo', 'quinto', 'porém', 'aos', 'seria', 'três', 'tempo', 'ela', 'puderam', 'dez', 'vezes', 'área', 'numa', 'quando', 'estas', 'fazes', 'meus', 'seis', 'ontem', 'pelo', 'mal', 'cujo', 'local', 'doze', 'fará', 'tem', 'porquê', 'vindo', 'oito', 'onze', 'aquela', 'conselho', 'que', 'vossas', 'tuas', 'vem', 'além', 'quais', 'maiorias', 'meu', 'nove', 'ser', 'maioria', 'da', 'estivemos', 'sem', 'umas', 'falta', 'é', 'nuns', 'obrigado', 'dos', 'nada', 'ali', 'tendes', 'dão', 'me', 'quem', 'direita', 'logo', 'podia', 'treze', 'maior', 'nós', 'posso', 'estiveste', 'poderá', 'ou', 'vossos', 'partir', 'dentro', 'outra', 'desse', 'estás', 'deverá', 'nova', 'eventual', 'cima', 'dizem', 'um', 'dezoito', 'teu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWKYCRDojNR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7753bbd-4dca-4cf6-adbb-9ccd42c67a2c"
      },
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6UGby2DjU64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ede0692-0b12-4697-af3e-5e025a21e734"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "len(stopwords)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84IyYSblRj2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd49653-139d-4ba4-def4-2b434f787848"
      },
      "source": [
        "stops_spacy_nltk = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "stops_spacy_nltk\n",
        "len(stops_spacy_nltk)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "499"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJtqP-KFjgem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e960998-d06f-40e4-e2cd-e6fb63ad82b0"
      },
      "source": [
        "#len(list(set(nlp.Defaults.stop_words) - set(stopwords)))\n",
        "list(set(nlp.Defaults.stop_words) - set(stopwords))[:10]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tentar',\n",
              " 'têm',\n",
              " 'tal',\n",
              " 'quatro',\n",
              " 'antes',\n",
              " 'qualquer',\n",
              " 'sistema',\n",
              " 'assim',\n",
              " 'lugar',\n",
              " 'mil']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euRE5EWEi7aI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6f6b10-f98b-44c0-d80c-f0e2aa2651d5"
      },
      "source": [
        "doc = nlp(u'pegar')\n",
        "token = doc[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pegar"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBTzb_DGVAii"
      },
      "source": [
        "#**Modelo de classificação**\n",
        "---\n",
        "\n",
        "<br>\n",
        "Vamos \"produtizar\" nosso melhor modelo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGvptdafSbj0",
        "outputId": "1fd4f20e-f4a7-42f5-d425-cf7617459922"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Carrega o dataframe\n",
        "df = pd.DataFrame({\n",
        "    'text': [\n",
        "      'Sobre MBA ? Eu gostei muito do MBA da FIAP',\n",
        "      'O MBA da FIAP pode melhorar, não gostei muito'\n",
        "    ],\n",
        "    'class': [\n",
        "        'positivo',\n",
        "        'negativo'\n",
        "    ]})\n",
        "\n",
        "# Vetoriza os documentos (cria um vetor treinado no contexto para o modelo)\n",
        "vect = TfidfVectorizer() \n",
        "vect.fit(df.text)\n",
        "tfidf_vect = vect.transform(df.text)\n",
        "\n",
        "print(pd.DataFrame(tfidf_vect.A, columns=vect.get_feature_names()).to_string())\n",
        "\n",
        "# Treina um modelo com os textos vetorizados\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(tfidf_vect, df['class'])\n",
        "\n",
        "# Valida as predições do modelo\n",
        "print('\\nD Tree: ', tree.score(tfidf_vect, df['class'])) # retorna a acurracy - precisão do modelo"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         da        do        eu      fiap    gostei       mba  melhorar     muito       não      pode     sobre\n",
            "0  0.267970  0.376623  0.376623  0.267970  0.267970  0.535941  0.000000  0.267970  0.000000  0.000000  0.376623\n",
            "1  0.302531  0.000000  0.000000  0.302531  0.302531  0.302531  0.425196  0.302531  0.425196  0.425196  0.000000\n",
            "\n",
            "D Tree:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR2ko_vNVCIO",
        "outputId": "8c2c4361-6710-4006-a85a-8d81aa818320"
      },
      "source": [
        "# Vetoriza um novo documento\n",
        "vetor = vect.transform(['o curso pode melhorar'])\n",
        "\n",
        "# Faz a previsão com base no modelo previamente treinado\n",
        "print('D Tree: ', tree.predict(vetor))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D Tree:  ['negativo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd0foInAVI4Q"
      },
      "source": [
        "Salvando nosso vetor treinado e nosso modelo de classificação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onRgUHxVVCGf"
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(vect, open('meu_vetor.pkl', 'wb'))\n",
        "pickle.dump(tree, open('minha_arvore.pkl', 'wb'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3AsyvcTVCEf",
        "outputId": "b53f1d7d-450b-4ee9-a8c8-6866dff7a50b"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 24\n",
            "drwxr-xr-x 1 root root 4096 May 10 17:55 .\n",
            "drwxr-xr-x 1 root root 4096 May 10 17:17 ..\n",
            "drwxr-xr-x 4 root root 4096 May  6 13:43 .config\n",
            "-rw-r--r-- 1 root root 1775 May 10 17:55 meu_vetor.pkl\n",
            "-rw-r--r-- 1 root root 1555 May 10 17:55 minha_arvore.pkl\n",
            "drwxr-xr-x 1 root root 4096 May  6 13:44 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCCCV1DzVCDA"
      },
      "source": [
        "import pickle\n",
        "\n",
        "bla_vetor = pickle.load(open('meu_vetor.pkl', 'rb'))\n",
        "bla_modelo = pickle.load(open('minha_arvore.pkl', 'rb'))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhLuIBNWVB-k",
        "outputId": "ee9081e6-8e4a-41b9-d7af-25c26628af5e"
      },
      "source": [
        "bla_vetor"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvU1n9vfVB8j",
        "outputId": "0b788f39-8718-41bf-d4c9-4681b0c7c7b3"
      },
      "source": [
        "bla_modelo"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNb8e1KmVB5u",
        "outputId": "4f980f1d-ffc4-4fa1-c26a-a0d4166279e1"
      },
      "source": [
        "# Vetoriza um novo documento com o vetor carregado\n",
        "novo_vetor = bla_vetor.transform(['o curso pode melhorar'])\n",
        "\n",
        "print(pd.DataFrame(novo_vetor.A, columns=bla_vetor.get_feature_names()).to_string())\n",
        "\n",
        "# Faz a previsão com base no modelo carregado\n",
        "print('D Tree: ', bla_modelo.predict(novo_vetor))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    da   do   eu  fiap  gostei  mba  melhorar  muito  não      pode  sobre\n",
            "0  0.0  0.0  0.0   0.0     0.0  0.0  0.707107    0.0  0.0  0.707107    0.0\n",
            "D Tree:  ['negativo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_twlhPF7VTkd"
      },
      "source": [
        "Salvando no Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na1yPJjrSbat",
        "outputId": "bde80b32-f1e4-4413-f376-28f75ba86cdf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# mostra a estrutura de pastas do google drive montado no colab\n",
        "!ls -la\n",
        "!ls -la /content/gdrive/MyDrive/FIAP/NLP/modelos"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "total 28\n",
            "drwxr-xr-x 1 root root 4096 May 10 17:56 .\n",
            "drwxr-xr-x 1 root root 4096 May 10 17:17 ..\n",
            "drwxr-xr-x 4 root root 4096 May  6 13:43 .config\n",
            "drwx------ 5 root root 4096 May 10 17:56 gdrive\n",
            "-rw-r--r-- 1 root root 1775 May 10 17:55 meu_vetor.pkl\n",
            "-rw-r--r-- 1 root root 1555 May 10 17:55 minha_arvore.pkl\n",
            "drwxr-xr-x 1 root root 4096 May  6 13:44 sample_data\n",
            "total 4\n",
            "-rw------- 1 root root 1772 Feb 20 00:23 meu_vetorizador.pkl\n",
            "-rw------- 1 root root 1555 Feb 20 00:23 minha_arvore.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozZhEoPWSbX8"
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(tree, open('/content/gdrive/MyDrive/FIAP/NLP/modelos/minha_arvore.pkl', 'wb'))\n",
        "pickle.dump(vect, open('/content/gdrive/MyDrive/FIAP/NLP/modelos/meu_vetorizador.pkl', 'wb'))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3FxYxPaSMCH",
        "outputId": "895935af-760c-4545-933a-c2657d1d40ae"
      },
      "source": [
        "!ls -la /content/gdrive/MyDrive/FIAP/NLP/modelos"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 1775 May 10 17:56 meu_vetorizador.pkl\n",
            "-rw------- 1 root root 1555 May 10 17:56 minha_arvore.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGS2QpmIVcUr"
      },
      "source": [
        "Reinicie o colab... <control>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd-z1RgBVZZL",
        "outputId": "a1950c6c-90a5-4868-9cbc-b9f5522fcd88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q90ce-pVZVg"
      },
      "source": [
        "import pickle\n",
        "\n",
        "minha_arvore = pickle.load(open('/content/gdrive/MyDrive/FIAP/NLP/modelos/minha_arvore.pkl', 'rb'))\n",
        "meu_vetorizador = pickle.load(open('/content/gdrive/MyDrive/FIAP/NLP/modelos/meu_vetorizador.pkl', 'rb'))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYGSne5hVZRz",
        "outputId": "8086d574-8965-42ee-8898-cf9900d6326b"
      },
      "source": [
        "# Vetoriza um novo documento com o vetor carregado\n",
        "novo_vetor = meu_vetorizador.transform(['o curso pode melhorar'])\n",
        "\n",
        "# Faz a previsão com base no modelo carregado\n",
        "print('D Tree: ', minha_arvore.predict(novo_vetor))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D Tree:  ['negativo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeL9MBwPVi0r"
      },
      "source": [
        "Escorando os documentos de um novo dataframe com as predições"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "zwDLrvb0VYx_",
        "outputId": "9ef8d426-130e-4580-ccd5-b56654b7da47"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Dataframe sem classificação\n",
        "df_novo = pd.DataFrame({\n",
        "    'texto': [\n",
        "      'Sobre MBA ? Eu gostei muito do MBA da FIAP',\n",
        "      'O MBA da FIAP pode melhorar, não gostei muito',\n",
        "      'O curso pode melhorar'\n",
        "    ]})\n",
        "\n",
        "df_novo"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sobre MBA ? Eu gostei muito do MBA da FIAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O MBA da FIAP pode melhorar, não gostei muito</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>O curso pode melhorar</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           texto\n",
              "0     Sobre MBA ? Eu gostei muito do MBA da FIAP\n",
              "1  O MBA da FIAP pode melhorar, não gostei muito\n",
              "2                          O curso pode melhorar"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZGpHzLzVYvn",
        "outputId": "423664c7-46a1-4595-faf2-8aecef27b3cc"
      },
      "source": [
        "texto_vetorizado = meu_vetorizador.transform(df_novo.texto)\n",
        "\n",
        "print(pd.DataFrame(texto_vetorizado.A, columns=meu_vetorizador.get_feature_names()).to_string())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         da        do        eu      fiap    gostei       mba  melhorar     muito       não      pode     sobre\n",
            "0  0.267970  0.376623  0.376623  0.267970  0.267970  0.535941  0.000000  0.267970  0.000000  0.000000  0.376623\n",
            "1  0.302531  0.000000  0.000000  0.302531  0.302531  0.302531  0.425196  0.302531  0.425196  0.425196  0.000000\n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107  0.000000  0.000000  0.707107  0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAHrcXcgVm28"
      },
      "source": [
        "df_novo['class_predict'] = minha_arvore.predict(texto_vetorizado)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "AKynjMOTVm0x",
        "outputId": "e13b3e47-0e60-497f-a78f-2d81b8eb28ee"
      },
      "source": [
        "df_novo"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>class_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sobre MBA ? Eu gostei muito do MBA da FIAP</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O MBA da FIAP pode melhorar, não gostei muito</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>O curso pode melhorar</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           texto class_predict\n",
              "0     Sobre MBA ? Eu gostei muito do MBA da FIAP      positivo\n",
              "1  O MBA da FIAP pode melhorar, não gostei muito      negativo\n",
              "2                          O curso pode melhorar      negativo"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMpK0H06Vysa"
      },
      "source": [
        "### **Exercício 1**\n",
        "\n",
        "###Que tal fazer isso com seu melhor modelo classificador de produtos?\n",
        "\n",
        "- Crie um pipeline produtivo do seu melhor modelo até aqui."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAQofki-VmyD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32luCbaKV4ko"
      },
      "source": [
        "#**Outros modelos de classificação**\n",
        "---\n",
        "<br>\n",
        "\n",
        "###O ***Scikit-Learn*** nos permite explorar mais modelos de classificação além da Árvore de Decisão (DecisionTreeClassifier), veja:\n",
        "<br>\n",
        "\n",
        "- Regressão Logistica (LogisticRegression)\n",
        "- Random Forest (RandomForestClassifier)\n",
        "- Naive Bayes (MultinomialNB e BernoulliNB)\n",
        "\n",
        "Entre outros..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHJhQ4QwVmtD"
      },
      "source": [
        "# Modelos de Regressão\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Random Forest (baseados em Ávores de Decisões)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Naive Bayes, bastante utilizado para classificar textos baseado na frequência das palavras independnetemente do contexto (classificação de SPAM)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdsbc8EqV_OP"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/linear_model.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRMIr0ReWCIE"
      },
      "source": [
        "Exemplo com Regressão Logística:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcXVMEG_V6-t",
        "outputId": "758edc36-bbe4-42eb-b5c0-9657a8badd3c"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# carrega os dados\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']\n",
        "\n",
        "# divisão das amostras em treino e teste\n",
        "df_treino, df_teste = train_test_split(\n",
        "      df, \n",
        "      test_size = 0.3, \n",
        "      random_state = 42\n",
        "  )\n",
        "\n",
        "#df.shape\n",
        "#df_treino.shape\n",
        "#df_teste.shape\n",
        "\n",
        "## TREINAMENTO\n",
        "# vetorização do dataframe de treino\n",
        "vect = CountVectorizer(ngram_range=(1,1)) # vetorização por contagem de termos simples dos unigramas com stopwords\n",
        "vect.fit(df_treino.texto)\n",
        "text_vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(text_vect_treino, df_treino.categoria)\n",
        "\n",
        "## VALIDAÇÃO\n",
        "# vetorização do dataframe de teste\n",
        "text_vect_teste = vect.transform(df_teste.texto)\n",
        "\n",
        "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
        "y_predicao = model.predict(text_vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia\n",
        "y_teste = df_teste.categoria\n",
        "accuracy = accuracy_score(y_predicao, y_teste) # predito x classificação original\n",
        "print(accuracy)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9805714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5lcBFbQD4ve"
      },
      "source": [
        "### **Exercício 2**\n",
        "\n",
        "Dado o dataset de produtos [1], descubra:\n",
        "- Treine deferentes modelos de classificação do pacote scikit-learn para classificar os produtos em suas categorias.\n",
        "- Experimente a lib SpaCy para remover as stop words e reduzir as palavras ao seu lema. Veja como essas alterações impactam o desempenho do classificador.\n",
        "- Compare a performance entre eles usando a métrica de acurácia.\n",
        "- Use randon_state igual a 42 para permitir a comparação com seus colegas, separando 30% para teste.\n",
        "\n",
        "\n",
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY3PjYltDrbx"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-VnZKnqULdR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ORAfF9QUMRh"
      },
      "source": [
        "# **SpaCy**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnj4fXwvURfu"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Ih55KpUS2u"
      },
      "source": [
        "#!python -m spacy download en\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!python -m spacy download pt_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IukkB7hUku9M"
      },
      "source": [
        "## Correspondência Baseada em Regras \n",
        "\n",
        "O spaCy oferece uma ferramenta de correspondência de regras chamada Matcher, que permite criar uma biblioteca de padrões de token e, em seguida, associar esses padrões a um objeto Doc para retornar uma lista de correspondências encontradas. Você pode combinar em qualquer parte do token, incluindo texto e anotações, e você pode adicionar vários padrões ao mesmo combinador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uutHbv8UkGaT"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trlgtXz5lJ6J"
      },
      "source": [
        "Aqui matcher é um objeto que é emparelhado com o objeto Vocab atual. Podemos adicionar e remover matchers nomeados específicos para o matcher, conforme necessário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiJM5rJ1loqQ"
      },
      "source": [
        "Podemos encontrar o termo \"guarda-chuva\" como uma palavra ou duas, com ou sem um hífen. Nesta seção, vamos desenvolver um matcher que encontre todos os três:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iITO-2i-lBCH"
      },
      "source": [
        "pattern1 = [{'LOWER': 'guardachuva'}]\n",
        "pattern2 = [{'LOWER': 'guarda'}, {'LOWER': 'chuva'}]\n",
        "pattern3 = [{'LOWER': 'guarda'}, {'IS_PUNCT': True}, {'LOWER': 'chuva'}]\n",
        "\n",
        "matcher.add('GuardaChuva', None, pattern1, pattern2, pattern3)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5TSsvpsmDvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1587bf0f-1176-4b6f-fee3-9198f079984d"
      },
      "source": [
        "doc = nlp('Hoje eu esqueci meu guardachuva. \\\n",
        "Vou ter que comprar um novo guarda - chuva. \\\n",
        "Quanto custa um guarda chuva?')\n",
        "\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(12789480426693079439, 4, 5), (12789480426693079439, 12, 15), (12789480426693079439, 19, 21)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3t8-Nw1fVFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f0e3fe-5f3d-43a5-c141-b1992cf530ed"
      },
      "source": [
        "doc[4:5]\n",
        "doc[12:15]\n",
        "doc[19:21]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "guarda chuva"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD4o3trJmflv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b34548-972a-4c88-f64c-0d074890f5ed"
      },
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    span = doc[start:end]\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12789480426693079439 GuardaChuva 4 5 guardachuva\n",
            "12789480426693079439 GuardaChuva 12 15 guarda - chuva\n",
            "12789480426693079439 GuardaChuva 19 21 guarda chuva\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts7j9EGOhK-q"
      },
      "source": [
        "É possível usar opções de POS Tag e o Lema dos termos como no exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT0aMgDWbpsT"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{'POS': 'VERB'}]\n",
        "\n",
        "matcher.add('Personalida', None, pattern)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRyd7YOgcbug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573183c7-bac9-4616-9b5c-b94d5be90af7"
      },
      "source": [
        "doc = nlp('O presidente Barak Obama visitou o Brasil')\n",
        "\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(9998440168381091967, 4, 5)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLD9iHbmmUN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3f05b0-395e-4748-fa0f-1c8f8f8e9fb2"
      },
      "source": [
        "doc[4:5]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "visitou"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf2xl7j3TEu7"
      },
      "source": [
        "Outro exemplo combinando regras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK_AJaJaTC9a"
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{'TEXT': 'andar', 'POS': 'NOUN'}]\n",
        "\n",
        "matcher.add('Regra 1', None, pattern)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulaw8TyvTC7v",
        "outputId": "b481cb1a-5de1-4765-c07c-8b635e9b0cf0"
      },
      "source": [
        "doc = nlp('Vamos andar até a esquina e depois subir para o terceiro andar do prédio.')\n",
        "\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(6205818778458672271, 11, 12)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY1zGqlPTC32",
        "outputId": "e8816ad5-1987-4278-ef4d-4e8382e997a5"
      },
      "source": [
        "[(token.text, token.orth_, token.pos_, token.dep_, spacy.explain(token.pos_)) for token in doc]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Vamos', 'Vamos', 'AUX', 'aux', 'auxiliary'),\n",
              " ('andar', 'andar', 'VERB', 'ROOT', 'verb'),\n",
              " ('até', 'até', 'ADP', 'case', 'adposition'),\n",
              " ('a', 'a', 'DET', 'det', 'determiner'),\n",
              " ('esquina', 'esquina', 'NOUN', 'obl', 'noun'),\n",
              " ('e', 'e', 'CCONJ', 'cc', 'coordinating conjunction'),\n",
              " ('depois', 'depois', 'ADV', 'advmod', 'adverb'),\n",
              " ('subir', 'subir', 'VERB', 'conj', 'verb'),\n",
              " ('para', 'para', 'ADP', 'case', 'adposition'),\n",
              " ('o', 'o', 'DET', 'det', 'determiner'),\n",
              " ('terceiro', 'terceiro', 'ADJ', 'amod', 'adjective'),\n",
              " ('andar', 'andar', 'NOUN', 'obl', 'noun'),\n",
              " ('do', 'do', 'ADV', 'case', 'adverb'),\n",
              " ('prédio', 'prédio', 'NOUN', 'nmod', 'noun'),\n",
              " ('.', '.', 'PUNCT', 'punct', 'punctuation')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2qY-3uTnFWA"
      },
      "source": [
        "Os seguintes quantificadores podem ser passados para a chave `'OP'`:\n",
        "\n",
        "<table><tr><th>OP</th><th>Descrição</th></tr>\n",
        "\n",
        "<tr ><td><span >\\!</span></td><td>Nega o padrão, exigindo que ele corresponda exatamente 0 vezes</td></tr>\n",
        "<tr ><td><span >?</span></td><td>Torna o padrão opcional, permitindo que ele corresponda 0 ou 1 vezes</td></tr>\n",
        "<tr ><td><span >\\+</span></td><td>Exige que o padrão corresponda a uma ou mais vezes</td></tr>\n",
        "<tr ><td><span >\\*</span></td><td>Permite que o padrão corresponda a zero ou mais vezes</td></tr>\n",
        "</table>\n",
        "\n",
        "**Outros atributos de token**\n",
        "\n",
        "<table><tr><th>Atributo</th><th>Descrição</th></tr>\n",
        "\n",
        "<tr ><td><span >`ORTH`</span></td><td>O texto exato do token</td></tr>\n",
        "<tr ><td><span >`LOWER`</span></td><td>O texto em caixa baixa</td></tr>\n",
        "<tr ><td><span >`LENGTH`</span></td><td>O tamanho do texto do token</td></tr>\n",
        "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>O texto do token consiste de alfanuméricos, ASCII, digitos</td></tr>\n",
        "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>O texto do toen esta em  lowercase, uppercase, titlecase</td></tr>\n",
        "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token é puntuação, espaço, stop-word</td></tr>\n",
        "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Texto do token se parece um numero, URL, email</td></tr>\n",
        "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>O token em sua representação de POS Tag, dependência, </td></tr>\n",
        "<tr ><td><span >`ENT_TYPE`</span></td><td>O tipo de entidade do token</td></tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "\n",
        "Para saber mais sobre esta função da lib SpaCy: \n",
        "\n",
        "https://spacy.io/api/matcher\n",
        "\n",
        "https://spacy.io/usage/rule-based-matching\n",
        "\n",
        "https://spacy.io/usage/linguistic-features#section-rule-based-matching"
      ]
    }
  ]
}